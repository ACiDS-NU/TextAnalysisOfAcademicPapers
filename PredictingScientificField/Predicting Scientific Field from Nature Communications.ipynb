{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing some basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrape data from Nature Communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrapeNatureComm(subject,num_articles):\n",
    "    # for a given subject (physical sciences, biological sciences, earth and environmental sciences, or health sciences)\n",
    "    # extract the article summaries for the most recent num_articles\n",
    "    \n",
    "    \n",
    "    # get html code from first page\n",
    "    url = 'https://www.nature.com/subjects/' + subject + '/ncomms'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # w contains the classes of the text description of the articles \n",
    "    w = ['mt4', 'serif', 'text13', 'tighten-line-height', 'text-gray', 'suppress-bottom-margin', 'hide-overflow', 'inline']\n",
    "    \n",
    "    # an array to contain all the text descriptions\n",
    "    summary = []\n",
    "    \n",
    "    # add the relavent data to the array\n",
    "    a = soup.find_all('div')\n",
    "    for i in a:\n",
    "        if i['class'] == w:\n",
    "            summary.append(i.text)\n",
    "    \n",
    "    # check if we have enough data\n",
    "    if len(summary) >= num_articles:\n",
    "        return summary[:num_articles]\n",
    "    \n",
    "    # start scraping pages\n",
    "    page_number = 2\n",
    "       \n",
    "    # loop to keep scraping until we reach the appropriate number of articles\n",
    "    while len(summary) <= num_articles:\n",
    "        \n",
    "        # get html code\n",
    "        url = 'https://www.nature.com/subjects/' + subject + '/ncomms?searchType=journalSearch&sort=PubDate&page=' + str(page_number)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # extract the relavent data\n",
    "        tempa = soup.find_all('div')\n",
    "        page_number += 1\n",
    "        for i in a:\n",
    "            if i['class'] == w:\n",
    "                summary.append(i.text)\n",
    "                \n",
    "    return summary[:num_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape physical science data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "physical = scrapeNatureComm('physical-sciences',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape biological science data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bio = scrapeNatureComm('biological-sciences',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape earth and environmental science data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enviro = scrapeNatureComm('earth-and-environmental-sciences',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape health science data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "health = scrapeNatureComm('health-sciences',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many articles have multiple classifications, so we need to remove duplicates and mark that they are have those classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all the physical articles are physical\n",
    "\n",
    "target_physical = [1] * len(physical)\n",
    "\n",
    "# by default, we'll assume that the other articles are not physical\n",
    "target_bio = [0] * len(physical)\n",
    "target_enviro = [0] * len(physical)\n",
    "target_health = [0] * len(physical)\n",
    "\n",
    "# check if a physical article has another id, if so remove that redundancy and mark that it has another id\n",
    "for i in range(len(physical)):\n",
    "\n",
    "    for j in range(len(bio)):\n",
    "        if physical[i] == bio[j]:\n",
    "            target_bio[i] = 1\n",
    "            del bio[j]\n",
    "            break\n",
    "            \n",
    "    for j in range(len(enviro)):\n",
    "        if physical[i] == enviro[j]:\n",
    "            target_enviro[i] = 1\n",
    "            del enviro[j]\n",
    "            break\n",
    "            \n",
    "    for j in range(len(health)):\n",
    "        if physical[i] == health[j]:\n",
    "            target_health[i] = 1\n",
    "            del health[j]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeat it for another classification\n",
    "\n",
    "target_physical += [0]*len(bio)\n",
    "target_bio += [1]*len(bio)\n",
    "target_enviro += [0]*len(bio)\n",
    "target_health += [0]*len(bio)\n",
    "\n",
    "for i in range(len(bio)):\n",
    "    \n",
    "    for j in range(len(enviro)):\n",
    "        if bio[i] == enviro[j]:\n",
    "            target_enviro[i+1000] = 1\n",
    "            del enviro[j]\n",
    "            break\n",
    "            \n",
    "    for j in range(len(health)):\n",
    "        if bio[i] == health[j]:\n",
    "            target_health[i+1000] = 1\n",
    "            del health[j]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and finally\n",
    "\n",
    "target_physical += [0]*len(enviro)\n",
    "target_bio += [0]*len(enviro)\n",
    "target_enviro += [1]*len(enviro)\n",
    "target_health += [0]*len(enviro)\n",
    "\n",
    "for i in range(len(enviro)):\n",
    "            \n",
    "    for j in range(len(health)):\n",
    "        if enviro[i] == health[j]:\n",
    "            target_health[i+1000+len(bio)] = 1\n",
    "            del health[j]\n",
    "            break\n",
    "            \n",
    "target_physical += [0]*len(health)\n",
    "target_bio += [0]*len(health)\n",
    "target_enviro += [0]*len(health)\n",
    "target_health += [1]*len(health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected the data now we can process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "combined = physical + bio + enviro + health\n",
    "\n",
    "# vectorize the text data\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(combined)\n",
    "\n",
    "# convert to a dataframe\n",
    "total_data = pd.DataFrame(X.toarray(),columns = vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit it to a Gaussian naive Bayes classifier. We'll consider each classification separately, and look at how the model does from cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation performance:\n",
      "Physical sciences model: [ 1.          0.97641509  1.          0.99055118  1.        ]\n",
      "Biological sciences model: [ 1.  1.  1.  1.  1.]\n",
      "Earth and Enviromental sciences model: [ 0.96698113  1.          1.          1.          1.        ]\n",
      "Health sciences model: [ 0.93710692  0.96855346  1.          0.98110236  0.9984252 ]\n"
     ]
    }
   ],
   "source": [
    "# importing a few relevant sklearn packages\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# evaluating cross validation\n",
    "print('Cross validation performance:')\n",
    "print('Physical sciences model: ' + str(cross_val_score(GaussianNB(),total_data.as_matrix(),(target_physical),cv = 5)))\n",
    "print('Biological sciences model: ' + str(cross_val_score(GaussianNB(),total_data.as_matrix(),(target_bio),cv = 5)))\n",
    "print('Earth and Enviromental sciences model: ' + str(cross_val_score(GaussianNB(),total_data.as_matrix(),(target_enviro),cv = 5)))\n",
    "print('Health sciences model: ' + str(cross_val_score(GaussianNB(),total_data.as_matrix(),(target_health),cv = 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phys_model = GaussianNB()\n",
    "phys_model.fit(total_data.as_matrix(),target_physical)\n",
    "\n",
    "bio_model = GaussianNB()\n",
    "bio_model.fit(total_data.as_matrix(),target_bio)\n",
    "\n",
    "enviro_model = GaussianNB()\n",
    "enviro_model.fit(total_data.as_matrix(),target_enviro)\n",
    "\n",
    "health_model = GaussianNB()\n",
    "health_model.fit(total_data.as_matrix(),target_health)\n",
    "\n",
    "# combining them into a list\n",
    "models = [phys_model,bio_model,enviro_model,health_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I wrote a function that will evaluate a string of text and see which scientific fields it might relate to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PredictScientificField(text,models,vec):\n",
    "    # given a string, the list of models generated, and the vectorizer used to generate the models\n",
    "    # it returns a string of the relavent scientific fields\n",
    "    \n",
    "    # vectorize input string\n",
    "    vector_text = vec.transform([text]).toarray()\n",
    "    \n",
    "    # initialize output string, notable it has a length of 29 so, if it's longer something has been added to it\n",
    "    output = 'Predicted Scientific Fields: '\n",
    "    \n",
    "    # the actual model fitting, commas are included if there is more than one field\n",
    "    if models[0].predict(vector_text) == 1:\n",
    "        output += 'Physical'\n",
    "        \n",
    "    if models[1].predict(vector_text) == 1:\n",
    "        if len(output) > 29:\n",
    "            output += ', Biological'\n",
    "        else:\n",
    "            output += 'Biological'\n",
    "            \n",
    "    if models[2].predict(vector_text) == 1:\n",
    "        if len(output) > 29:\n",
    "            output += ', Eath/Environmental' \n",
    "        else:\n",
    "            output += 'Eath/Environmental'\n",
    "            \n",
    "    if models[3].predict(vector_text) == 1:\n",
    "        if len(output) > 29:\n",
    "            output += ', Health'\n",
    "        else:\n",
    "            output += 'Health'\n",
    "    \n",
    "    # if no fields have been identified, it says none\n",
    "    if len(output) == 29:\n",
    "        output += 'None'\n",
    "    \n",
    "    # print and return the output\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the function at work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Scientific Fields: Biological, Eath/Environmental\n"
     ]
    }
   ],
   "source": [
    "PredictScientificField('Rocks, climate change, cells ',models,vec);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
